{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinkedIn post scraper and RAG chat bot analyzer\n",
    "\n",
    "**This concept was developed using Python 3.10.16 in an Anaconda environment.**\n",
    "\n",
    "```\n",
    "conda create -n osint python=3.10  \n",
    "conda activate osint\n",
    "```\n",
    "\n",
    "**This concept demonstrates which data we share on LinkedIn and how easily it can be analyzed and misused.**  \n",
    "**Please use it only as a tool for your own social network hygiene.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (6.29.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (8.35.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (24.2)\n",
      "Requirement already satisfied: psutil in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel) (4.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: undetected-chromedriver in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (3.5.5)\n",
      "Requirement already satisfied: selenium>=4.9.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from undetected-chromedriver) (4.31.0)\n",
      "Requirement already satisfied: requests in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from undetected-chromedriver) (2.32.3)\n",
      "Requirement already satisfied: websockets in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: trio~=0.17 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from requests->undetected-chromedriver) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from requests->undetected-chromedriver) (3.10)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: outcome in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
      "Requirement already satisfied: trio~=0.17 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement time (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for time\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from bs4) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from beautifulsoup4->bs4) (4.13.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (1.72.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (5.24.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.115.9)\n",
      "Requirement already satisfied: ffmpy in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (1.8.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.30.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (2.2.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (3.10.16)\n",
      "Requirement already satisfied: packaging in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (2.11.3)\n",
      "Requirement already satisfied: pydub in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.11.5)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (4.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: fsspec in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (1.0.4)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (2.11.3)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (2.2.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (3.24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (4.13.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (1.32.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (1.32.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (0.53b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (1.32.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: packaging>=19.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: anyio in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.53b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/siva01/miniconda3/envs/osint/lib/python3.10/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel\n",
    "!pip install undetected-chromedriver\n",
    "!pip install selenium\n",
    "!pip install time\n",
    "!pip install bs4\n",
    "!pip install pandas\n",
    "!pip install openai\n",
    "!pip install gradio\n",
    "!pip install chromadb\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from urllib.parse import urlparse\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sleep(min_seconds=5, max_seconds=30):\n",
    "    \"\"\"\n",
    "    Sleep for a random amount of time between min and max seconds.\n",
    "    This helps avoid detection by mimicking human behavior.\n",
    "    \"\"\"\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Cookies\n",
    "\n",
    "Saving cookies while web scraping serves several important purposes:\n",
    "\n",
    "1. **Authentication Persistence**: Maintains login state between sessions, avoiding repeated logins\n",
    "2. **Reduced Detection Risk**: Makes requests appear more like a regular user by maintaining consistent cookies\n",
    "3. **Efficiency**: Saves time by reusing authenticated sessions rather than logging in each time\n",
    "4. **Rate Limiting**: Helps manage rate limits by maintaining consistent session identification\n",
    "5. **Compliance**: Some websites require specific cookies for tracking user agreements and preferences\n",
    "\n",
    "The `save_cookies()` function stores these essential browser cookies to a text file for later reuse in subsequent scraping sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cookies(driver, filename=\"linkedin_cookies.txt\"):\n",
    "    \"\"\"Save cookies to a file for later use\"\"\"\n",
    "    if not os.path.exists(\"cookies\"):\n",
    "        os.makedirs(\"cookies\")\n",
    "    \n",
    "    path = os.path.join(\"cookies\", filename)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for cookie in driver.get_cookies():\n",
    "            f.write(f\"{cookie['name']}={cookie['value']}\\n\")\n",
    "            print(f\"Cookies saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cookies(driver, filename=\"linkedin_cookies.txt\") -> bool:\n",
    "    \"\"\"Load cookies from a file and add them to the browser session\"\"\"\n",
    "    try:\n",
    "        path = os.path.join(\"cookies\", filename)\n",
    "        if os.path.exists(path):\n",
    "            # First navigate to LinkedIn domain to set cookies properly\n",
    "            driver.get(\"https://www.linkedin.com\")\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                cookies = f.readlines()\n",
    "                for cookie in cookies:\n",
    "                    name, value = cookie.strip().split('=', 1)\n",
    "                    cookie_dict = {\n",
    "                        'name': name,\n",
    "                        'value': value,\n",
    "                        'domain': '.linkedin.com'  # Set proper domain for LinkedIn cookies\n",
    "                    }\n",
    "                    try:\n",
    "                        driver.add_cookie(cookie_dict)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error adding cookie {name}: {e}\")\n",
    "                        continue\n",
    "            print(f\"Cookies loaded from {path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Cookie file not found at {path}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cookies: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome executable path in my Linux, replace with your own path (run which chromium-browser in terminal)\n",
    "chrome_path = \"/usr/bin/chromium-browser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome browser settings: \n",
    "# headless mode is used to run the browser in the background without a GUI. It doesn't work with lazy loading\n",
    "def get_driver(headless=False) -> \"uc.Chrome | None\": \n",
    "    options = uc.ChromeOptions()\n",
    "    if headless:\n",
    "         options.add_argument(\"--headless\")  # Run in headless mode\n",
    "\n",
    "    else:\n",
    "        # Disagle GPU acceleration\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "        # Disable sandboxing\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "    \n",
    "        # Disable shared memory space\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "        # It helps bypass anti-bot detection systems that look for automated browser signatures\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        \n",
    "        # Open Chrome in a maximized window\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "    \n",
    "        # This is try, how to disable saving passwords, privacy settings, etc.\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--disable-infobars\")\n",
    "        options.add_argument(\"--disable-popup-blocking\")\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--disable-translate\")\n",
    "        options.add_argument(\"--disable-application-cache\")\n",
    "        options.add_argument(\"--disable-extensions-file-access-check\")\n",
    "        options.add_argument(\"--disable-extensions-http-throttling\")\n",
    "        prefs = {\n",
    "          \"credentials_enable_service\": False,\n",
    "          \"profile.password_manager_enabled\": False,\n",
    "          \"autofill.profile_enabled\": False,\n",
    "          \"autofill.enabled\": False,\n",
    "          \"profile.default_content_setting_values.notifications\": 2,\n",
    "          \"profile.managed_default_content_settings.popups\": 2\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        options.add_argument(\"--password-store=basic\")\n",
    "        options.add_argument(\"--disable-features=PasswordManager\")\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = uc.Chrome(options=options, version_main=134)  # Specify your Chrome major version\n",
    "        driver.start_session()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    return driver    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening Chrome Browser & Log in to LinkedIn\n",
    "\n",
    "Do not interact with the Chrome browser. All actions are performed automatically by the scripts below.\n",
    "\n",
    "driver = new Chrome browser window\n",
    "\n",
    "#### Set get_driver(True) for head less mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_driver(headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "undetected_chromedriver.Chrome"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If driver is working, we can quit it and use get_driver method in login function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is only for logging into a LinkedIn account. To log in, you need an account email and password. Use your own credentials or a test account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_login(username, password) -> \"uc.Chrome | None\":  \n",
    "    \"\"\"\n",
    "    Function to login to LinkedIn using undetected_chromedriver\n",
    "    \n",
    "    Parameters:\n",
    "    username (str): Your LinkedIn email/username\n",
    "    password (str): Your LinkedIn password\n",
    "    \n",
    "    Returns:\n",
    "    webdriver: Browser instance logged into LinkedIn\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "     \n",
    "    if load_cookies(driver):\n",
    "        # Navigate to LinkedIn feed page\n",
    "        driver.get(\"https://www.linkedin.com\")\n",
    "        random_sleep(2, 4)\n",
    "        driver.get(\"https://www.linkedin.com/feed/\")\n",
    "        # Check if we're already logged in\n",
    "        if \"/feed\" in driver.current_url:\n",
    "            print(\"Already logged in to LinkedIn!\")\n",
    "            return driver\n",
    "        \n",
    "        # If not logged in, clear cookies and start fresh\n",
    "        driver.delete_all_cookies()\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            # Navigate to LinkedIn login page\n",
    "            driver.get(\"https://www.linkedin.com/login\")\n",
    "            random_sleep(2, 4)\n",
    "        \n",
    "            # Wait for the page to load and username field to be present\n",
    "            username_field = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"username\"))\n",
    "            )\n",
    "        \n",
    "            # Enter username with random typing speed to mimic human behavior\n",
    "            for char in username:\n",
    "                username_field.send_keys(char)\n",
    "                random_sleep(0.1, 0.3)\n",
    "                \n",
    "            random_sleep(3,6)\n",
    "            \n",
    "            # Enter password with random typing speed\n",
    "            password_field = driver.find_element(By.ID, \"password\")\n",
    "            for char in password:\n",
    "                password_field.send_keys(char)\n",
    "                random_sleep(0.1, 0.3)\n",
    "        \n",
    "            random_sleep(2, 4)\n",
    "        \n",
    "            # Click login button\n",
    "            login_button = driver.find_element(By.XPATH, \"//button[contains(@class, 'btn__primary--large')]\")\n",
    "            login_button.click()\n",
    "        \n",
    "            # Wait 30 seconds for login to complete and verify we're on the feed page\n",
    "            try:\n",
    "                WebDriverWait(driver, 30).until(\n",
    "                    lambda driver: urlparse(driver.current_url).netloc == \"www.linkedin.com\" and \n",
    "                    (\"/feed\" in driver.current_url or \"/checkpoint\" in driver.current_url)\n",
    "                    )\n",
    "                \n",
    "                # Check if we've been redirected to a security checkpoint\n",
    "                if \"/checkpoint\" in driver.current_url:\n",
    "                    print(\"LinkedIn security checkpoint detected. Manual intervention may be required.\")\n",
    "                    # Allow time for manual intervention if needed\n",
    "                    input(\"Press Enter after completing the security checkpoint...\")\n",
    "                else:\n",
    "                    print(\"Successfully logged in to LinkedIn!\")    \n",
    "    \n",
    "                save_cookies(driver)\n",
    "                return driver\n",
    "            \n",
    "            except TimeoutException:\n",
    "                print(\"Login unsuccessful or redirected to an unexpected page, or it take more than 30 seconds.\")\n",
    "                print(f\"Current URL: {driver.current_url}\")\n",
    "                driver.quit()\n",
    "                \n",
    "            return None\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during login: {e}\")\n",
    "            driver.quit()\n",
    "\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .env File\n",
    "\n",
    "Copy .env.default to .env, edit .env file and set your LinkedIn username and password. Then update the target name and target username with the account you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of copy the .env.default file to .env: \n",
    "# !cp .env.default .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_username = os.getenv(\"LINKEDIN_USER\")\n",
    "linkedin_password = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "target_name = os.getenv(\"LINKEDIN_TARGET_NAME\")\n",
    "target_username = os.getenv(\"LINKEDIN_TARGET_USERNAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, Login\n",
    "\n",
    "Now I will use linkedin_username and linkedin_password defined above to invoke the linkedin_login function. You can run it and watch your Chrome browser to see what the script is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookies loaded from cookies/linkedin_cookies.txt\n",
      "Already logged in to LinkedIn!\n",
      "Logged in to LinkedIn successfully: https://www.linkedin.com/feed/\n"
     ]
    }
   ],
   "source": [
    " # Use existing driver instance\n",
    "linkedin = linkedin_login(linkedin_username, linkedin_password)\n",
    "\n",
    "if linkedin is None:\n",
    "    print(\"Failed to log in to LinkedIn.\")\n",
    "else:\n",
    "    print(f\"Logged in to LinkedIn successfully: {linkedin.current_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**  \n",
    "If you get the error `\"An error occurred during login: 'NoneType' object is not iterable\"`, it means you skipped the part where the `.env` file is created and the credentials and target are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ludekkvapil'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just check, if I have correct taget username\n",
    "target_username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let Chrome open the target user's posts URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin.get(f\"https://www.linkedin.com/in/{target_username}/recent-activity/all/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.linkedin.com/in/ludekkvapil/recent-activity/all/'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkedin.current_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Loading Posts  \n",
    "When we open the feed, not all posts are loaded initially. We need to scroll down and wait until the posts are fully loaded. This function does it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_load_post(max_scrolls=1000) -> bool:\n",
    "    \"\"\"\n",
    "    Improved scrolling with incremental approach and better detection\n",
    "    \"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    posts_count = len(driver.find_elements(By.CLASS_NAME, \"feed-shared-update-v2__control-menu-container\"))\n",
    "    \n",
    "    for i in range(max_scrolls):\n",
    "        # Scroll down incrementally (1/4 of viewport height at a time)\n",
    "        driver.execute_script(\"window.scrollBy(0, window.innerHeight/4);\")\n",
    "        \n",
    "        # Wait for new content to load with random timing\n",
    "        random_sleep(2, 4)  # Shorter initial wait\n",
    "        \n",
    "        # After a few incremental scrolls, check if new content loaded\n",
    "        if i % 4 == 3:  # Check after every 4 small scrolls\n",
    "            # Count visible posts\n",
    "            new_posts_count = len(driver.find_elements(By.CLASS_NAME, \"feed-shared-update-v2__control-menu-container\"))\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            print(f\"Scroll progress: {i+1}/{max_scrolls}, Posts: {new_posts_count}, Height: {new_height}\")\n",
    "            \n",
    "            # If no new posts appeared after multiple scroll attempts\n",
    "            if new_posts_count == posts_count and new_height == last_height:\n",
    "                # Try one big scroll as a last attempt\n",
    "                driver.execute_script(\"window.scrollBy(0, window.innerHeight*2);\")\n",
    "                random_sleep(5, 10)\n",
    "                \n",
    "                # Check again\n",
    "                newer_posts_count = len(driver.find_elements(By.CLASS_NAME, \"feed-shared-update-v2__control-menu-container\"))\n",
    "                if newer_posts_count == new_posts_count:\n",
    "                    print(f\"No new content loaded after {i+1} scrolls. Stopping.\")\n",
    "                    driver.execute_script(\"window.scrollTo(0, 0);\")  # Scroll back to top\n",
    "                    return True\n",
    "            \n",
    "            # Update tracking variables\n",
    "            posts_count = new_posts_count\n",
    "            last_height = new_height\n",
    "    \n",
    "    print(f\"Reached maximum number of scrolls ({max_scrolls}).\")\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")  # Scroll back to top\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the lazy load function, and it will perform the scrolling magic in your Chrome browser. It can take several minutes, depending on the number of posts the target has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scroll progress: 4/1000, Posts: 7, Height: 4835\n",
      "Scroll progress: 8/1000, Posts: 10, Height: 5105\n",
      "Scroll progress: 12/1000, Posts: 13, Height: 5817\n",
      "Scroll progress: 16/1000, Posts: 15, Height: 6438\n",
      "Scroll progress: 20/1000, Posts: 17, Height: 6662\n",
      "Scroll progress: 24/1000, Posts: 20, Height: 7553\n",
      "Scroll progress: 28/1000, Posts: 23, Height: 11200\n",
      "Scroll progress: 32/1000, Posts: 25, Height: 11424\n",
      "Scroll progress: 36/1000, Posts: 28, Height: 11817\n",
      "Scroll progress: 40/1000, Posts: 31, Height: 12701\n",
      "Scroll progress: 44/1000, Posts: 32, Height: 12813\n",
      "Scroll progress: 48/1000, Posts: 35, Height: 13756\n",
      "Scroll progress: 52/1000, Posts: 36, Height: 14161\n",
      "Scroll progress: 56/1000, Posts: 39, Height: 15099\n",
      "Scroll progress: 60/1000, Posts: 40, Height: 15158\n",
      "Scroll progress: 64/1000, Posts: 43, Height: 18690\n",
      "Scroll progress: 68/1000, Posts: 44, Height: 19391\n",
      "Scroll progress: 72/1000, Posts: 47, Height: 20603\n",
      "Scroll progress: 76/1000, Posts: 48, Height: 21324\n",
      "Scroll progress: 80/1000, Posts: 49, Height: 21891\n",
      "Scroll progress: 84/1000, Posts: 50, Height: 22087\n",
      "Scroll progress: 88/1000, Posts: 53, Height: 22501\n",
      "Scroll progress: 92/1000, Posts: 57, Height: 23505\n",
      "Scroll progress: 96/1000, Posts: 58, Height: 23654\n",
      "Scroll progress: 100/1000, Posts: 59, Height: 24314\n",
      "Scroll progress: 104/1000, Posts: 63, Height: 28521\n",
      "Scroll progress: 108/1000, Posts: 63, Height: 28521\n",
      "Scroll progress: 112/1000, Posts: 73, Height: 29643\n",
      "Scroll progress: 116/1000, Posts: 75, Height: 30130\n",
      "Scroll progress: 120/1000, Posts: 78, Height: 33404\n",
      "Scroll progress: 124/1000, Posts: 80, Height: 33548\n",
      "Scroll progress: 128/1000, Posts: 82, Height: 34035\n",
      "Scroll progress: 132/1000, Posts: 86, Height: 34624\n",
      "Scroll progress: 136/1000, Posts: 88, Height: 34865\n",
      "Scroll progress: 140/1000, Posts: 91, Height: 35436\n",
      "Scroll progress: 144/1000, Posts: 95, Height: 38429\n",
      "Scroll progress: 148/1000, Posts: 96, Height: 39075\n",
      "Scroll progress: 152/1000, Posts: 97, Height: 39204\n",
      "Scroll progress: 156/1000, Posts: 100, Height: 39676\n",
      "Scroll progress: 160/1000, Posts: 103, Height: 40143\n",
      "Scroll progress: 164/1000, Posts: 106, Height: 40698\n",
      "Scroll progress: 168/1000, Posts: 109, Height: 44045\n",
      "Scroll progress: 172/1000, Posts: 113, Height: 44420\n",
      "Scroll progress: 176/1000, Posts: 117, Height: 44989\n",
      "Scroll progress: 180/1000, Posts: 120, Height: 45346\n",
      "Scroll progress: 184/1000, Posts: 123, Height: 45480\n",
      "Scroll progress: 188/1000, Posts: 126, Height: 45835\n",
      "Scroll progress: 192/1000, Posts: 131, Height: 48805\n",
      "Scroll progress: 196/1000, Posts: 136, Height: 49083\n",
      "Scroll progress: 200/1000, Posts: 139, Height: 49532\n",
      "Scroll progress: 204/1000, Posts: 142, Height: 50029\n",
      "Scroll progress: 208/1000, Posts: 143, Height: 50068\n",
      "Scroll progress: 212/1000, Posts: 149, Height: 53180\n",
      "Scroll progress: 216/1000, Posts: 154, Height: 53465\n",
      "Scroll progress: 220/1000, Posts: 156, Height: 53910\n",
      "Scroll progress: 224/1000, Posts: 158, Height: 54574\n",
      "Scroll progress: 228/1000, Posts: 161, Height: 54794\n",
      "Scroll progress: 232/1000, Posts: 164, Height: 58419\n",
      "Scroll progress: 236/1000, Posts: 169, Height: 58627\n",
      "Scroll progress: 240/1000, Posts: 173, Height: 58756\n",
      "Scroll progress: 244/1000, Posts: 178, Height: 58957\n",
      "Scroll progress: 248/1000, Posts: 183, Height: 61881\n",
      "Scroll progress: 252/1000, Posts: 187, Height: 62017\n",
      "Scroll progress: 256/1000, Posts: 190, Height: 62379\n",
      "Scroll progress: 260/1000, Posts: 195, Height: 62631\n",
      "Scroll progress: 264/1000, Posts: 200, Height: 65783\n",
      "Scroll progress: 268/1000, Posts: 204, Height: 66029\n",
      "Scroll progress: 272/1000, Posts: 208, Height: 66275\n",
      "Scroll progress: 276/1000, Posts: 213, Height: 66390\n",
      "Scroll progress: 280/1000, Posts: 216, Height: 66480\n",
      "Scroll progress: 284/1000, Posts: 222, Height: 69475\n",
      "Scroll progress: 288/1000, Posts: 226, Height: 69743\n",
      "Scroll progress: 292/1000, Posts: 231, Height: 69995\n",
      "Scroll progress: 296/1000, Posts: 234, Height: 70158\n",
      "Scroll progress: 300/1000, Posts: 237, Height: 73643\n",
      "Scroll progress: 304/1000, Posts: 240, Height: 74061\n",
      "Scroll progress: 308/1000, Posts: 243, Height: 74833\n",
      "Scroll progress: 312/1000, Posts: 246, Height: 74890\n",
      "Scroll progress: 316/1000, Posts: 251, Height: 75228\n",
      "Scroll progress: 320/1000, Posts: 255, Height: 77269\n",
      "Scroll progress: 324/1000, Posts: 260, Height: 77521\n",
      "Scroll progress: 328/1000, Posts: 264, Height: 77657\n",
      "Scroll progress: 332/1000, Posts: 264, Height: 77585\n",
      "Scroll progress: 336/1000, Posts: 264, Height: 77585\n",
      "No new content loaded after 336 scrolls. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy_load_post()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get post \n",
    "This function search for CSS elements and map them as post data and add them to list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts() -> dict:\n",
    "    \"\"\"\n",
    "    Improved post extraction with multiple selector strategies and better error handling\n",
    "    \"\"\"\n",
    "    posts = {}\n",
    "    \n",
    "    # Try multiple selector strategies\n",
    "    post_containers = driver.find_elements(By.CLASS_NAME, \"feed-shared-update-v2__control-menu-container\")\n",
    "    print(f\"Found {len(post_containers)} potential post containers\")\n",
    "    \n",
    "    for idx, element in enumerate(post_containers):\n",
    "        try:\n",
    "            # Try first approach - standard post\n",
    "            try:\n",
    "                post_element = element.find_element(By.CLASS_NAME, \"update-components-text\")\n",
    "                post_element = post_element.find_element(By.CSS_SELECTOR, \"span[dir='ltr']\").get_attribute(\"innerHTML\")\n",
    "                metadata = element.find_element(By.CSS_SELECTOR, \"span.update-components-actor__sub-description.text-body-xsmall.t-black--light\")\n",
    "                metadata = metadata.find_element(By.CSS_SELECTOR, \"span.visually-hidden\").get_attribute(\"innerHTML\")\n",
    "            except NoSuchElementException:\n",
    "                # Try alternative selectors for different post types\n",
    "                try:\n",
    "                    # Look for any text content with fallbacks\n",
    "                    post_element = element.find_element(By.CSS_SELECTOR, \".feed-shared-update-v2__description-wrapper\").get_attribute(\"innerHTML\")\n",
    "                    metadata_element = element.find_element(By.CSS_SELECTOR, \".feed-shared-actor__meta\")\n",
    "                    metadata = metadata_element.text\n",
    "                except NoSuchElementException:\n",
    "                    # Skip this post if we can't find text\n",
    "                    print(f\"Skipping post #{idx} - structure not recognized\")\n",
    "                    # print(f\"Post HTML: {element.get_attribute('innerHTML')}\")\n",
    "                    continue\n",
    "            \n",
    "            # Clean and process the text\n",
    "            text = BeautifulSoup(post_element, \"html.parser\").get_text(\"\\n\", strip=True)\n",
    "            metadata = BeautifulSoup(metadata, \"html.parser\").get_text(\"\\n\", strip=True)\n",
    "            \n",
    "            # Skip empty posts\n",
    "            if not text.strip():\n",
    "                print(f\"Skipping empty post #{idx}\")\n",
    "                continue\n",
    "                \n",
    "            unique_key = f\"{abs(hash(metadata + text))}\"\n",
    "            posts[unique_key] = {\n",
    "                \"user\": target_name,\n",
    "                \"metadata\": metadata, \n",
    "                \"text\": text\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post #{idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully extracted {len(posts)} posts out of {len(post_containers)} containers\")\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout scrapped posts, uncomment the line below to see them.\n",
    "# get_posts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store it to JSON File  \n",
    "For future use, I’d like to have the posts stored in a file called `posts.json`. It will be created in the root folder of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 264 potential post containers\n",
      "Skipping post #41 - structure not recognized\n",
      "Skipping post #73 - structure not recognized\n",
      "Skipping post #74 - structure not recognized\n",
      "Skipping post #239 - structure not recognized\n",
      "Successfully extracted 260 posts out of 264 containers\n",
      "Posts saved to posts.json\n"
     ]
    }
   ],
   "source": [
    "# Get posts\n",
    "posts = get_posts()\n",
    "\n",
    "# Save posts to a JSON file\n",
    "with open(\"posts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(posts, f, ensure_ascii=False, indent=4)\n",
    "   \n",
    "print(\"Posts saved to posts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need Chrome browser anymore. It works only for headless mode\n",
    "if linkedin:\n",
    "    linkedin.quit()\n",
    "elif driver:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Using GenAI: LinkedIn Posts RAG Chatbot  \n",
    "\n",
    "In this section, I use ChromaDB to store posts as vectors. Then, I use OpenAI to analyze the retrieved data and display the results in a Gradio chat window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Application  \n",
    "\n",
    "This object is responsible for embedding, storing, retrieving, and answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagChat:\n",
    "    def __init__(self, json_path: str = 'posts.json', collection_name: str = 'knowledge_base'):\n",
    "        \"\"\"\n",
    "        Initialize the RAG application with a JSON file and ChromaDB collection\n",
    "        \n",
    "        :param json_path: Path to the JSON file containing knowledge base\n",
    "        :param collection_name: Name of the ChromaDB collection\n",
    "        \"\"\"\n",
    "        # Initialize ChromaDB client with specific folder and index, \n",
    "        # default ChromaDb embedding model.    \n",
    "        os.makedirs(\"./chromadb\", exist_ok=True)\n",
    "        self.chroma_client = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "        self.linkedin = self.chroma_client.get_or_create_collection(\n",
    "            name=\"linkedin_posts\"\n",
    "        )\n",
    "        \n",
    "        # Load and embed JSON data\n",
    "        self.data = self.load_json_data(json_path)\n",
    "        self.index_data(json_path)\n",
    "        \n",
    "\n",
    "    def index_data(self, json_path: str):\n",
    "        if self.data is not None:\n",
    "            print(\"Data loaded successfully.\")\n",
    "        \n",
    "            for idx, (post_id, post_data) in enumerate(self.data.items()):\n",
    "                chroma_id = f\"topic_{idx}\"\n",
    "            \n",
    "                # Check if ID already exists in the collection\n",
    "                existing = self.linkedin.get(ids=[chroma_id])\n",
    "                if existing and existing.get(\"ids\"):\n",
    "                   # print(f\"Skipping {chroma_id} (already exists in ChromaDB)\")\n",
    "                   continue  # Skip if already indexed\n",
    "            \n",
    "                # Create a comprehensive text document from the topic\n",
    "                document_parts = [\n",
    "                   f\"User: {post_data.get('user', '')}\",\n",
    "                   f\"Text: {post_data.get('text', '')}\",\n",
    "                   f\"Metadata: {post_data.get('metadata', '')}\"\n",
    "                ]\n",
    "                document = '\\n\\n'.join(document_parts)\n",
    "\n",
    "                metadata = {\n",
    "                    \"source\": json_path,\n",
    "                    \"index\": idx,\n",
    "                    \"title\": post_id\n",
    "                }\n",
    "\n",
    "                # Add the new document to ChromaDB\n",
    "                self.linkedin.add(\n",
    "                   documents=[document],\n",
    "                   metadatas=[metadata],\n",
    "                   ids=[chroma_id]\n",
    "                   )\n",
    "                # print(f\"Added {chroma_id} to ChromaDB\")\n",
    "\n",
    "            print(\"Indexing complete.\")\n",
    "        else:\n",
    "            print(\"No topics found in the JSON file.\")\n",
    "\n",
    "       \n",
    "    def load_json_data(self, json_path: str, strings_only: bool = False):\n",
    "        \"\"\"\n",
    "        Load JSON data and embed it into ChromaDB\n",
    "        :param json_path: Path to the JSON file\n",
    "        \"\"\"\n",
    "        # Read JSON file\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                if strings_only:\n",
    "                    return data\n",
    "                else:\n",
    "                    # Load the entire JSON structure\n",
    "                    data = json.load(f)\n",
    "                    # print(f\"Data found {data} \" )\n",
    "                    return data\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {json_path} not found.\")\n",
    "            return\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Invalid JSON format in {json_path}.\")\n",
    "            return\n",
    "        \n",
    "    \n",
    "    def get_prompt_category(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Categorize the query to determine the context to retrieve\n",
    "        :param prompt User's query\n",
    "        :return: Category of the query\n",
    "        \"\"\"\n",
    "        system_prompt = f\"\"\"\n",
    "        You are a personal assistant who categorizes queries.\n",
    "        You are given a query and you need to categorize it.\n",
    "        The categories are:\n",
    "        1. summary: The query is asking for a summary, all posts, or full text analysis\n",
    "        2. default: Rest of the queries\n",
    "\n",
    "        The query is: {prompt}\n",
    "        Based on the query, return the category.\n",
    "        Do not make up any answers.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        category = self.chat_completion(system_prompt, prompt)\n",
    "\n",
    "        if \"summary\" in category.lower():\n",
    "            return \"summary\"\n",
    "        else:\n",
    "            return \"default\"\n",
    "\n",
    "    def retrieve_context(self, query: str, top_k: int = 10) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieve most relevant context for a given query\n",
    "        \n",
    "        :param query: User's query\n",
    "        :param top_k: Number of top results to retrieve\n",
    "        :return: List of retrieved context documents\n",
    "        \"\"\"\n",
    "        # Retrieve top K most similar documents\n",
    "        results = self.linkedin.query(\n",
    "            query_texts=[query],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        \n",
    "        # Extract and parse documents\n",
    "        contexts = []\n",
    "        if 'documents' in results and len(results['documents']) > 0:\n",
    "            for doc in results['documents'][0]:\n",
    "                contexts.append(doc)\n",
    "\n",
    "        print(f\"Context: {contexts} \")\n",
    "        return contexts\n",
    "    \n",
    "    def chat_completion(self, system_prompt: str, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a chat completion using OpenAI\n",
    "        :param  system_prompt: System prompt\n",
    "        :param  prompt: User's query\n",
    "        :return: Generated response\n",
    "        \"\"\"\n",
    "        print(f\"System prompt: {system_prompt}\")\n",
    "        print(f\"User prompt: {prompt}\")\n",
    "\n",
    "        try:\n",
    "            # Generate response using OpenAI\n",
    "            completion = openai.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Access the response correctly based on OpenAI API version\n",
    "            if hasattr(completion, 'choices') and len(completion.choices) > 0:\n",
    "                if hasattr(completion.choices[0], 'message'):\n",
    "                    return completion.choices[0].message.content\n",
    "                else:\n",
    "                    return completion.choices[0].text\n",
    "            return \"No response generated\"\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred while generating the response: {str(e)}\"\n",
    "\n",
    "    def generate_response(self, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response using retrieved context and OpenAI\n",
    "        \n",
    "        :param user_prompt: User's query\n",
    "        :return: Generated response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            category = self.get_prompt_category(user_prompt)\n",
    "            print(f\"Query category: {category}\")\n",
    "\n",
    "            if category == \"summary\":\n",
    "                # Convert JSON data to list of strings for the summary case\n",
    "                contexts = []\n",
    "                for post_id, post_data in self.data.items():\n",
    "                    document_parts = [\n",
    "                        f\"User: {post_data.get('user', '')}\",\n",
    "                        f\"Text: {post_data.get('text', '')}\",\n",
    "                        f\"Metadata: {post_data.get('metadata', '')}\"\n",
    "                        ]\n",
    "                    contexts.append('\\n\\n'.join(document_parts))\n",
    "         \n",
    "            else:\n",
    "                # Retrieve context based on the user's query\n",
    "                contexts = self.retrieve_context(user_prompt)\n",
    "\n",
    "            \n",
    "            if contexts:\n",
    "                print(f\"Retrieved {len(contexts)} contexts\")\n",
    "            else:\n",
    "                print(\"No relevant contexts found\")\n",
    "\n",
    "            system_prompt = f\"\"\".\n",
    "            You are a personal assistant who is analyzing LinkedIn posts written by {target_name}\n",
    "            Do not make up any answers. \n",
    "            If user ask about he, or she, is is question about {target_name}.\n",
    "            Use knowledge base to answer the question.\n",
    "            If you don't have enough information from knowledge base, say \"I don't know\".\n",
    "\n",
    "            Knowledge base: \n",
    "            {' '.join(contexts)}\n",
    "            \"\"\"\n",
    "        \n",
    "            return self.chat_completion(system_prompt, user_prompt)\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"An error occurred while generating the response: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Indexing complete.\n"
     ]
    }
   ],
   "source": [
    "# Loading scraped posts from JSON file to ChromaDb\n",
    "rag_app = RagChat('posts.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of RAG chat with dataset based on scraped LinkedIn posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Does {target_name} have any experience with Drupal?\n",
      "System prompt: \n",
      "        You are a personal assistant who categorizes queries.\n",
      "        You are given a query and you need to categorize it.\n",
      "        The categories are:\n",
      "        1. summary: The query is asking for a summary, all posts, or full text analysis\n",
      "        2. default: Rest of the queries\n",
      "\n",
      "        The query is: Does {target_name} have any experience with Drupal?\n",
      "        Based on the query, return the category.\n",
      "        Do not make up any answers.\n",
      "\n",
      "        \n",
      "User prompt: Does {target_name} have any experience with Drupal?\n",
      "Query category: default\n",
      "Context: ['User: Ludek Kvapil\\n\\nText: To, že někdo  umí číst a psát v PHP, ještě neznamená, že je to programátor.\\n\\nMetadata: 8 years ago • Visible to anyone on or off LinkedIn', \"User: Ludek Kvapil\\n\\nText: I am developing a RAG chatbot - a tool that enhances LLM knowledge with specific, private, or less-known information. Currently, I have a PDF and JSON processor. I'm considering which knowledge sources are essential for such an application.\\nhashtag\\n#\\nRAG\\nhashtag\\n#\\nAI\\nhashtag\\n#\\nchatbot\\n\\nMetadata: 1 week ago • Visible to anyone on or off LinkedIn\", \"User: Ludek Kvapil\\n\\nText: ### I Created a Weapon\\nI'm working on my personal RAG chatbot and got the idea that it would be useful to include some LinkedIn posts in the knowledge base. So, I built a scraper that collects all posts from my profile and saves them to a JSON file. In the next step, I use ChromaDB as a vector store for that text, allowing me to search it as vectors. The data is then used by an OpenAI-based chatbot, which can search and analyze the posts.\\nThis concept is great for analyzing your posts and improving your social media hygiene, but it can also be misused for malicious purposes like scams, manipulation, or doxing.\\nTechnologies used:\\nhashtag\\n#\\nPython\\n,\\nhashtag\\n#\\nJupyterLab\\n,\\nhashtag\\n#\\nSelenium\\n,\\nhashtag\\n#\\nChromaDB\\n,\\nhashtag\\n#\\nOpenAI\\n, and\\nhashtag\\n#\\nGradio\\n.\\nUse this\\nhashtag\\n#\\nOSINT\\n&\\nhashtag\\n#\\nGenAI\\ntool wisely.\\n\\nMetadata: 5 hours ago • Visible to anyone on or off LinkedIn\", 'User: Ludek Kvapil\\n\\nText: I teaching AI how create custom Drupal module\\nhashtag\\n#\\nOpenAI\\nhashtag\\n#\\nDrupal\\n\\nMetadata: 2 years ago • Visible to anyone on or off LinkedIn', 'User: Ludek Kvapil\\n\\nText: Máme nové LinkedIn. Design neřeším. Pro mě je důležitý výkon. A nové LinkedIn je znatelně pomalejší.\\n\\nMetadata: 8 years ago • Visible to anyone on or off LinkedIn', \"User: Ludek Kvapil\\n\\nText: Internalization of entity_load query\\nI am using Taxonomy Formatter for multilingual website. Taxonomy term is loaded by entity_load().  Is can´t find how get translated term name. I could translate it directly in translation interface, but it isn´t conceptual approach. All others term I have translated by Taxonomy translation module.\\nSo do you know how customize this code?\\nforeach ($items as $delta => $item) {\\n$termid = $item['tid'];\\n$term[] = $termid;\\n$actterm = entity_load('taxonomy_term', $term);\\nif(isset($actterm[$termid])) {\\n$uri = entity_uri('taxonomy_term', $actterm[$termid]);\\nif ($settings['links_option']) {\\n$formatted .= $elementwrap[0] . l($actterm[$termid]->name, $uri['path'], $uri['options']) . $elementwrap[1] . $separator;\\n}\\nelse {\\n$formatted .= $elementwrap[0] . check_plain($actterm[$termid]->name) . $elementwrap[1] . $separator;\\n}\\n}\\n}\\n\\nMetadata: 8 years ago • Only visible to members of Drupal\", 'User: Ludek Kvapil\\n\\nText: I tried the concept of an\\nhashtag\\n#\\nAI\\npriest, and it looks like it can work.\\nAn AI priest can teach the concept of religion. People are still important as emotional support, but concepts can be taught by AI.\\n\\nMetadata: 2 years ago • Visible to anyone on or off LinkedIn', 'User: Ludek Kvapil\\n\\nText: Plugins can extend the functionalities of ChatGPT. Integrating with platforms such as Wolfram and Zapier can have a significant impact.\\n\\nMetadata: 2 years ago • Visible to anyone on or off LinkedIn', 'User: Ludek Kvapil\\n\\nText: ### Git Blame\\nGit Blame is just a command, but there are big differences in how people use it. I think it’s great when you find a bug and need to understand the context of why it occurred.\\nOn the other hand, I’ve met people who use it to find the author of a bug and blame them. That only leads to a toxic work environment.\\nWe should use Git Blame only to understand the context of a bug and find a way to fix it.\\n\\nMetadata: 4 days ago • Visible to anyone on or off LinkedIn', 'User: Ludek Kvapil\\n\\nText: The worst performance metric ever: the number of lines of code.\\n\\nMetadata: 1 month ago • Visible to anyone on or off LinkedIn'] \n",
      "Retrieved 10 contexts\n",
      "System prompt: .\n",
      "            You are a personal assistant who is analyzing LinkedIn posts written by Ludek Kvapil\n",
      "            Do not make up any answers. \n",
      "            If user ask about he, or she, is is question about Ludek Kvapil.\n",
      "            Use knowledge base to answer the question.\n",
      "            If you don't have enough information from knowledge base, say \"I don't know\".\n",
      "\n",
      "            Knowledge base: \n",
      "            User: Ludek Kvapil\n",
      "\n",
      "Text: To, že někdo  umí číst a psát v PHP, ještě neznamená, že je to programátor.\n",
      "\n",
      "Metadata: 8 years ago • Visible to anyone on or off LinkedIn User: Ludek Kvapil\n",
      "\n",
      "Text: I am developing a RAG chatbot - a tool that enhances LLM knowledge with specific, private, or less-known information. Currently, I have a PDF and JSON processor. I'm considering which knowledge sources are essential for such an application.\n",
      "hashtag\n",
      "#\n",
      "RAG\n",
      "hashtag\n",
      "#\n",
      "AI\n",
      "hashtag\n",
      "#\n",
      "chatbot\n",
      "\n",
      "Metadata: 1 week ago • Visible to anyone on or off LinkedIn User: Ludek Kvapil\n",
      "\n",
      "Text: ### I Created a Weapon\n",
      "I'm working on my personal RAG chatbot and got the idea that it would be useful to include some LinkedIn posts in the knowledge base. So, I built a scraper that collects all posts from my profile and saves them to a JSON file. In the next step, I use ChromaDB as a vector store for that text, allowing me to search it as vectors. The data is then used by an OpenAI-based chatbot, which can search and analyze the posts.\n",
      "This concept is great for analyzing your posts and improving your social media hygiene, but it can also be misused for malicious purposes like scams, manipulation, or doxing.\n",
      "Technologies used:\n",
      "hashtag\n",
      "#\n",
      "Python\n",
      ",\n",
      "hashtag\n",
      "#\n",
      "JupyterLab\n",
      ",\n",
      "hashtag\n",
      "#\n",
      "Selenium\n",
      ",\n",
      "hashtag\n",
      "#\n",
      "ChromaDB\n",
      ",\n",
      "hashtag\n",
      "#\n",
      "OpenAI\n",
      ", and\n",
      "hashtag\n",
      "#\n",
      "Gradio\n",
      ".\n",
      "Use this\n",
      "hashtag\n",
      "#\n",
      "OSINT\n",
      "&\n",
      "hashtag\n",
      "#\n",
      "GenAI\n",
      "tool wisely.\n",
      "\n",
      "Metadata: 5 hours ago • Visible to anyone on or off LinkedIn User: Ludek Kvapil\n",
      "\n",
      "Text: I teaching AI how create custom Drupal module\n",
      "hashtag\n",
      "#\n",
      "OpenAI\n",
      "hashtag\n",
      "#\n",
      "Drupal\n",
      "\n",
      "Metadata: 2 years ago • Visible to anyone on or off LinkedIn User: Ludek Kvapil\n",
      "\n",
      "Text: Máme nové LinkedIn. Design neřeším. Pro mě je důležitý výkon. A nové LinkedIn je znatelně pomalejší.\n",
      "\n",
      "Metadata: 8 years ago • Visible to anyone on or off LinkedIn User: Ludek Kvapil\n",
      "\n",
      "Text: Internalization of entity_load query\n",
      "I am using Taxonomy Formatter for multilingual website. Taxonomy term is loaded by entity_load().  Is can´t find how get translated term name. I could translate it directly in translation interface, but it isn´t conceptual approach. All others term I have translated by Taxonomy translation module.\n",
      "So do you know how customize this code?\n",
      "foreach ($items as $delta => $item) {\n",
      "$termid = $item['tid'];\n",
      "$term[] = $termid;\n",
      "$actterm = entity_load('taxonomy_term', $term);\n",
      "if(isset($actterm[$termid])) {\n",
      "$uri = entity_uri('taxonomy_term', $actterm[$termid]);\n",
      "if ($settings['links_option']) {\n",
      "$formatted .= $elementwrap[0] . l($actterm[$termid]->name, $uri['path'], $uri['options']) . $elementwrap[1] . $separator;\n",
      "}\n",
      "else {\n",
      "$formatted .= $elementwrap[0] . check_plain($actterm[$termid]->name) . $elementwrap[1] . $separator;\n",
      "}\n",
      "}\n",
      "}\n",
      "\n",
      "Metadata: 8 years ago • Only visible to members of Drupal User: Ludek Kvapil\n",
      "\n",
      "Text: I tried the concept of an\n",
      "hashtag\n",
      "#\n",
      "AI\n",
      "priest, and it looks like it can work.\n",
      "An AI priest can teach the concept of religion. People are still important as emotional support, but concepts can be taught by AI.\n",
      "\n",
      "Metadata: 2 years ago • Visible to anyone on or off LinkedIn User: Ludek Kvapil\n",
      "\n",
      "Text: Plugins can extend the functionalities of ChatGPT. Integrating with platforms such as Wolfram and Zapier can have a significant impact.\n",
      "\n",
      "Metadata: 2 years ago • Visible to anyone on or off LinkedIn User: Ludek Kvapil\n",
      "\n",
      "Text: ### Git Blame\n",
      "Git Blame is just a command, but there are big differences in how people use it. I think it’s great when you find a bug and need to understand the context of why it occurred.\n",
      "On the other hand, I’ve met people who use it to find the author of a bug and blame them. That only leads to a toxic work environment.\n",
      "We should use Git Blame only to understand the context of a bug and find a way to fix it.\n",
      "\n",
      "Metadata: 4 days ago • Visible to anyone on or off LinkedIn User: Ludek Kvapil\n",
      "\n",
      "Text: The worst performance metric ever: the number of lines of code.\n",
      "\n",
      "Metadata: 1 month ago • Visible to anyone on or off LinkedIn\n",
      "            \n",
      "User prompt: Does {target_name} have any experience with Drupal?\n",
      "Response: Yes, Ludek Kvapil has experience with Drupal. He mentioned teaching AI how to create a custom Drupal module.\n"
     ]
    }
   ],
   "source": [
    "# Test questions\n",
    "queries = [\n",
    "    \"Does {target_name} have any experience with Drupal?\",\n",
    "]\n",
    "    \n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    response = rag_app.generate_response(query)\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Gradio \n",
    "Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then share a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just helper function which allow me use RAG chat object in Gradio\n",
    "def respond(message, history):\n",
    "    # Generate response using the RAG object\n",
    "    bot_message = rag_app.generate_response(message)\n",
    "    # Return rag bot message\n",
    "    return bot_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"RAG Knowledge Base Chat\",\n",
    "    description=\"Ask questions about the content in your posts.json file\",\n",
    "    examples=[\"Show me all posts about OSINT\"],\n",
    "    theme=\"monochrome\", # Theme list: monochrome, default, soft\n",
    "    chatbot=gr.Chatbot(type=\"messages\"),\n",
    "    type=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Launch the Gradio app, set true for public link \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
